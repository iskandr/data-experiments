{
 "metadata": {
  "name": "regression"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 224
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def fprop(x,w):\n",
      "    return np.tanh(linear_fprop(x,w))\n",
      "\n",
      "def linear_fprop(x,w):\n",
      "    return np.dot(x,w)\n",
      "\n",
      "def gradient(x,y,w):\n",
      "    out = fprop(x,w)\n",
      "    if len(x.shape) == 1:\n",
      "      g = (1 - out**2) * x * (out - y)\n",
      "    else:\n",
      "      g = np.zeros_like(x[0])\n",
      "      for i, xi in enumerate(x):\n",
      "        g += (1 - out[i]**2) * xi * (out[i] - y[i])\n",
      "      g /= x.shape[0]\n",
      "    return g \n",
      "    \n",
      "\n",
      "def linear_gradient(x,y,w):\n",
      "    if len(x.shape) == 1:\n",
      "      return x * (linear_fprop(x,w) - y)\n",
      "    else:\n",
      "      g = np.zeros_like(x[0])\n",
      "      for i, xi in enumerate(x):\n",
      "        g += xi * (linear_fprop(xi, w) - y[i])\n",
      "      return g / x.shape[0]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 243
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nsamples = 10**5\n",
      "ndims = 100\n",
      "w_actual = np.random.randn(ndims)\n",
      "x = np.random.randn(nsamples, ndims)\n",
      "y = fprop(x,w_actual)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 244
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "w_init = np.random.randn(ndims)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 245
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sgd(x, y, w_init, learning_rate = .1, tol = 10.0 ** -4, maxiters = 10**4, show_plot = True, verbose = True):\n",
      "    curr_err = np.inf \n",
      "    i = 0\n",
      "    norms = []\n",
      "    errs = []\n",
      "    w = w_init.copy() \n",
      "    n_samples = len(x)\n",
      "    while curr_err > tol and i < maxiters:\n",
      "        i += 1\n",
      "        sample_id = i % n_samples\n",
      "        g = gradient(x[sample_id, :],y[sample_id],w)\n",
      "        norm = np.linalg.norm(g)\n",
      "        norms.append(norm)\n",
      "        w -= learning_rate * g\n",
      "        err = np.abs(fprop(x[sample_id, :], w) - y[sample_id])\n",
      "      \n",
      "        errs.append(err)\n",
      "        if verbose and (i % 1000 == 0):\n",
      "          curr_err = np.mean(np.abs(fprop(x,w) - y))\n",
      "          print \"Iter %d, training error for SGD %0.4f\" % (i, curr_err)\n",
      "    if show_plot:\n",
      "      plot(norms)\n",
      "      ylabel(\"norm\")\n",
      "      figure()\n",
      "      plot(errs)\n",
      "      ylabel(\"per-sample error\")\n",
      "    if verbose:\n",
      "      print \"Training error for SGD\", np.mean(np.abs(fprop(x, w) - y))\n",
      "    return i, w"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 303
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_ = sgd(x,y,w_init, show_plot = False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Iter 1000, training error for SGD 0.7343\n",
        "Iter 2000, training error for SGD 0.5404"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iter 3000, training error for SGD 0.3691"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iter 4000, training error for SGD 0.2912"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iter 5000, training error for SGD 0.2140"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iter 6000, training error for SGD 0.1850"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iter 7000, training error for SGD 0.1677"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iter 8000, training error for SGD 0.1544"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iter 9000, training error for SGD 0.1488"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iter 10000, training error for SGD 0.1344"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training error for SGD 0.134427057842\n"
       ]
      }
     ],
     "prompt_number": 304
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def par_sgd(x, y, w_init, n_workers, \n",
      "           local_learning_rate = 0.1, \n",
      "           global_learning_rate = 1, \n",
      "           tol = 10.0 ** -4, \n",
      "           maxiters = 100, \n",
      "           local_steps = 10,\n",
      "           newton = True, \n",
      "           avg_weights = False, \n",
      "           momentum = True, \n",
      "           verbose = True):\n",
      "    w_global = w_init \n",
      "    curr_err = np.inf\n",
      "    norms = []\n",
      "    i = 0\n",
      "    n_samples = len(x)\n",
      "  \n",
      "    while (curr_err > tol) and (i < maxiters):\n",
      "        i += 1\n",
      "        dws = []\n",
      "        dgs = []\n",
      "        local_weights = []\n",
      "        local_xs = []\n",
      "        local_ys = []\n",
      "        for worker_num in xrange(n_workers):\n",
      "            local_idx = np.random.randint(0, n_samples, local_steps)\n",
      "      \n",
      "            local_x = x[local_idx, :]\n",
      "            local_y = y[local_idx]\n",
      "            local_xs.append(local_x)\n",
      "            local_ys.append(local_y)\n",
      "            w = w_global.copy()\n",
      "            g_start = gradient(local_x, local_y, w)\n",
      "            \n",
      "            for sample_id in xrange(local_steps):\n",
      "                xi = local_x[sample_id, :]\n",
      "                yi = local_y[sample_id]\n",
      "                g = gradient(xi, yi, w)\n",
      "                w -= local_learning_rate * g\n",
      "            dg = gradient(local_x, local_y, w) - g_start \n",
      "            dw = w - w_global\n",
      "            dws.append(dw)\n",
      "            dgs.append(dg)\n",
      "            local_weights.append(w)\n",
      "        \n",
      "    \n",
      "        if avg_weights:\n",
      "          w_combined = np.mean(np.array(local_weights), axis = 0)\n",
      "          #w_combined = local_weights[0]\n",
      "        else:\n",
      "          best_err = np.inf \n",
      "          best_weight = None\n",
      "          val_idx = np.random.randint(0, n_samples, 10*local_steps)\n",
      "          val_x = x[val_idx]\n",
      "          val_y = y[val_idx]\n",
      "          for local_weight in local_weights:\n",
      "            local_err = np.mean(np.abs(fprop(val_x, local_weight) - val_y))\n",
      "            if local_err < best_err:\n",
      "                best_err = local_err\n",
      "                best_weight = local_weight\n",
      "          w_combined = best_weight\n",
      "        \n",
      "        local_gradients = []\n",
      "        for worker_num in xrange(n_workers):\n",
      "            val_idx = np.random.randint(0, n_samples, 100)\n",
      "            local_x = x[val_idx]\n",
      "            local_y = y[val_idx]\n",
      "            local_gradients.append(gradient(local_x, local_y, w_combined))\n",
      "        g = np.mean(np.array(local_gradients), axis=0)\n",
      "        if newton:\n",
      "          G = np.array(dgs)\n",
      "          W = np.array(dws)\n",
      "          U, D, Vt = np.linalg.svd(G.T, full_matrices=False)\n",
      "          Ug = np.dot(U.T, g)\n",
      "          DinvUg = np.dot(np.diag(1.0 / D), Ug)\n",
      "          VDinvUg = np.dot(Vt.T, DinvUg)  \n",
      "          search_dir = -np.dot(W.T, VDinvUg)\n",
      "        else:\n",
      "          search_dir = -g\n",
      "\n",
      "        norms.append(np.linalg.norm(search_dir))\n",
      "        \n",
      "        if global_learning_rate == 'search':\n",
      "            etas = 10.0 ** np.array([3,2.5,2,1.5, 1, 0.5, 0, -0.5, -1, -1.5, -2])\n",
      "            best_err = np.inf \n",
      "            best_weights = None\n",
      "            val_idx = np.random.randint(0, n_samples, 200)\n",
      "            val_x = x[val_idx]\n",
      "            val_y = y[val_idx]\n",
      "            best_eta = None\n",
      "            for eta in etas:\n",
      "                candidate = w_combined + eta * search_dir\n",
      "                err = np.mean(np.abs(fprop(val_x, candidate) - val_y))\n",
      "                \n",
      "                if err < best_err:\n",
      "                    best_eta = eta\n",
      "                    best_err = err\n",
      "                    best_weight = candidate\n",
      "            print \" -- chose eta = %s\" % best_eta \n",
      "            w_global = best_weight\n",
      "        else:\n",
      "          w_global = w_combined + global_learning_rate * search_dir\n",
      "        \n",
      "        if verbose:\n",
      "          curr_err = np.mean(np.abs(fprop(x, w_global) - y))\n",
      "          print \"Iter %d, grad_norm %0.4f, search_dir norm %0.4f, training error for SGD %0.4f\" % \\\n",
      "            (i, np.linalg.norm(g), np.linalg.norm(search_dir), curr_err)\n",
      "  \n",
      "    print \"Finished after %d iters\" % i \n",
      "    return i, w_global\n",
      "        \n",
      "         "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 341
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_ = par_sgd(x, y, w_init, 128, global_learning_rate = 'search', maxiters=20, newton=False, avg_weights = True, local_steps = 100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -- chose eta = 1000.0\n",
        "Iter 1, grad_norm 0.0684, search_dir norm 0.0684, training error for SGD 0.2360\n",
        " -- chose eta = 1000.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iter 2, grad_norm 0.0108, search_dir norm 0.0108, training error for SGD 0.1829\n",
        " -- chose eta = 1000.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iter 3, grad_norm 0.0106, search_dir norm 0.0106, training error for SGD 0.1292\n",
        " -- chose eta = 316.227766017"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iter 4, grad_norm 0.0108, search_dir norm 0.0108, training error for SGD 0.1100\n",
        " -- chose eta = 1000.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iter 5, grad_norm 0.0108, search_dir norm 0.0108, training error for SGD 0.0873\n",
        " -- chose eta = 316.227766017"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iter 6, grad_norm 0.0084, search_dir norm 0.0084, training error for SGD 0.0757\n",
        " -- chose eta = 31.6227766017"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iter 7, grad_norm 0.0090, search_dir norm 0.0090, training error for SGD 0.0738\n",
        " -- chose eta = 316.227766017"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iter 8, grad_norm 0.0080, search_dir norm 0.0080, training error for SGD 0.0644\n",
        " -- chose eta = 316.227766017"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iter 9, grad_norm 0.0054, search_dir norm 0.0054, training error for SGD 0.0603\n",
        " -- chose eta = 31.6227766017"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iter 10, grad_norm 0.0057, search_dir norm 0.0057, training error for SGD 0.0596\n",
        " -- chose eta = 316.227766017"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iter 11, grad_norm 0.0060, search_dir norm 0.0060, training error for SGD 0.0566\n",
        " -- chose eta = 3.16227766017"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iter 12, grad_norm 0.0038, search_dir norm 0.0038, training error for SGD 0.0565\n",
        " -- chose eta = 316.227766017"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iter 13, grad_norm 0.0042, search_dir norm 0.0042, training error for SGD 0.0556\n",
        " -- chose eta = 31.6227766017"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iter 14, grad_norm 0.0036, search_dir norm 0.0036, training error for SGD 0.0553\n",
        " -- chose eta = 100.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iter 15, grad_norm 0.0039, search_dir norm 0.0039, training error for SGD 0.0550\n",
        " -- chose eta = 100.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iter 16, grad_norm 0.0037, search_dir norm 0.0037, training error for SGD 0.0546\n",
        " -- chose eta = 100.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iter 17, grad_norm 0.0035, search_dir norm 0.0035, training error for SGD 0.0544\n",
        " -- chose eta = 0.01"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iter 18, grad_norm 0.0036, search_dir norm 0.0036, training error for SGD 0.0544\n",
        " -- chose eta = 316.227766017"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iter 19, grad_norm 0.0032, search_dir norm 0.0032, training error for SGD 0.0554\n",
        " -- chose eta = 316.227766017"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iter 20, grad_norm 0.0038, search_dir norm 0.0038, training error for SGD 0.0551\n",
        "Finished after 20 iters\n"
       ]
      }
     ],
     "prompt_number": 347
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.dot?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 327
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}